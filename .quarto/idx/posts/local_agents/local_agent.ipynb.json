{"title":"Building Agents with Tools Locally","markdown":{"yaml":{"title":"Building Agents with Tools Locally","date":"2025-02-21","description":"Building an AI Agent with tools locally using HuggingFace SmolAgents and Transformers Libraries","categories":["tutorial","code"],"image":"thumbnail.webp","image-alt":"V-LLM Generated Image","format":{"html":{"code-fold":false,"css":"../../styles.css","page-layout":"full","max-width":"1300px"}}},"headingText":"Introduction: Why AI Agents Matter","containsRefs":false,"markdown":"\n\n\n![V-LLM Generated Image](main.webp \"\")\n\n<br>\nBefore diving into the coding process, it's essential to understand the purpose of the program we aim to build. We need to identify the problem at hand and the business value we can derive from solving it...\n\nLol, who am I kidding, we're not at work here, so let's focus on picking problems that are ***fun*** to solve and let the marketing people worry about value creation.\n\nJokes aside, today our goal is to learn how to overcome the traditional limitations of Large Language Models (LLMs) by building Agents that are able to use *custom tools*.  \n\nWhile LLMs like GPT-4 are incredibly powerful and capable of performing various tasks such as answering questions, summarizing text, and reasoning about complex topics, they have **significant limitations** when used in isolation. What are those limitations, one may ask?\n\n\n1. **Lack of Real-Time Data**  \n   - LLMs are trained on static datasets and *cannot fetch live information* (e.g., stock prices, weather updates, or breaking news).  \n   - This leads to outdated or incomplete responses.  \n<br>\n2. **Inability to Perform Actions**  \n   - An LLM can tell you how to book a flight but *canâ€™t actually book it*â€”it lacks the ability to interact with external systems.  \n   - Agents, on the other hand, can call APIs, interact with databases, and automate tasks.  \n<br>\n3. **Limited Memory & Context**  \n   - While LLMs have a context window, they *forget past interactions beyond a limit* and struggle with long-term tasks.  \n   - Agents can store and retrieve relevant information, improving coherence over long workflows.  \n<br>\n4. **Mathematical & Computational Weaknesses**  \n   - LLMs can make **math mistakes** or fail in complex computations.  \n   - Agents can call external computation tools (e.g., a Python script or a calculator API) for precise results.  \n\n\n\n#### **How Agents Solve These Problems**  \n*AI agents* address many of the limitations of traditional language models by integrating external tools, allowing for greater accuracy, real-time data retrieval, and interaction with external systems. In this tutorial, we will explore how to create and integrate custom tools with LLMs to build more robust and functional AI agents. \n\nSpecifically, we will develop:\n\n- ðŸ§® Mathematical computation tools capable of producing precise results, overcoming the tendency of LLMs to generate incorrect numerical answers (with a clear example of ChatGPT's limitation that you can test against your Agent's tool) \n- ðŸ“ˆ Real-time stock market query tools to retrieve up-to-date financial information. \n- ðŸ” Web-search tools to access the latest research and trends, ensuring information is current. \n\nAll implementations will rely entirely on open-source libraries (big love to Hugging Face â¤ï¸) and will run locally on a standard machine with no more than 12GB of VRAM. While many existing tutorials focus on cloud-based implementations via [HuggingFace's Serverless API](https://huggingface.co/docs/api-inference/en/index), I want to demonstrate how to get things running without relying on the cloud.\n\n\n\n## Building Open Source Agents using HuggingFace's SmolAgents\n\n\nTo monitor GPU utilization in real-time, especially when running resource-intensive machine learning tasks, you can leverage the `nvidia-smi` command. On Ubuntu, simply uncomment and execute the following command in your terminal (provided that you have an Nvidia GPU):\n\n```bash\nwatch -n 1 nvidia-smi\n```\n\nThis command offers a live, updating snapshot of your GPU's performance, enabling you to monitor key metrics such as memory utilization and temperature. The `watch -n 1` component ensures that this information is refreshed every second. This is particularly useful for debugging, optimizing your machine learning workflows, and identifying specific commands or models that lead to critical \"Out Of Memory\" errors.\n\n### Imports & Setup\n\nIn this initial code block, we start by importing the necessary libraries to construct our local AI agent. Let's break down each import:\n\n- `typing`: We import `Any` from the `typing` module to provide type hints for our function signatures, which is a critical part of creating useful tools that our agents that use.\n\n- `smolagents`: From the `smolagents` library, we import:\n    - `Tool`: A base class for defining custom tools that our agent can use.\n    - `CodeAgent`: The core class for creating agents capable of generating and executing code.\n    - `TransformersModel`: A class to load and manage transformer models.\n    - `tool`: A decorator to easily register functions as tools for the agent.\n    - `DuckDuckGoSearchTool`: A pre-built tool that allows the agent to perform web searches using DuckDuckGo.\n- `transformers`: We import `AutoTokenizer` from the `transformers` library, which will be used to process text data for our models.\n- `yaml`: This library is imported to handle YAML configuration files, allowing us to define agent settings in a structured format.\n- `yfinance`: We import `yfinance` to fetch stock market data.\n- `datetime` and `pytz`: These are used for handling date and time information, which will be useful when working with financial data.\n\nThese imports set the stage for building a powerful AI agent that can interact with its environment through code execution and web searches.\n\n### Define your prompt template\nLoad up the adequate yaml prompt template. This one is compatible with the smolagents library and the model we will use. It was taken directly from [HuggingFace's Agents Course](https://huggingface.co/agents-course)\n\n### **Understanding AI Tool Creation with SmolAgents**\n\nWhen building AI agents, **defining tools properly is crucial** for ensuring smooth integration with frameworks like [SmolAgents](https://huggingface.co/docs/smolagents/en/index).  \nThe first tool we will build today uses the `yfinance` library to check the status (price) of a given ticker on the North American stock market. \n\n#### **Creating your first tool**\nThe function **`check_NA_market_status`** is decorated with `@tool`, marking it as a callable tool within the agent framework. This tool checks whether the stock market is open and returns the latest stock price if open, or the last closing price if closed.\n\n1. **Appropriate type-hinting is Essential**  \n   - The function takes a **single string argument (`ticker: str`)** and returns a **string (`-> str`)**.  \n   - This typing ensures that the agent understands the expected input and output format.  \n   - Without this, the function may not work correctly in an AI pipeline.\n\n2. **The Importance of a Docstring**  \n   - The function is documented with a **clear, structured docstring** explaining:  \n     - **What it does** (checks if the market is open and returns the appropriate stock price).  \n     - **What arguments it takes** (`ticker` â€“ the stock ticker symbol).  \n     - **What it returns** (a formatted string with market status and price).  \n   - AI agents rely on this information to understand the functionâ€™s role. When/if you build your own tool, it is critical to keep the same docstring template for your code to be compatible with the smolagents library.\n\n\n\n##### **Key Takeaways for developping tools**\n \n\n- **Follow the function template:** Use clear type hints and structured docstrings.  \n- **Ensure proper decoration:** The `@tool` decorator registers the function within the SmolAgents framework.  \n- **Keep return types simple:** AI models parse these results, so returning formatted strings helps with interpretability.  \n\nThis structured approach makes it easier for Agents to *call tools correctly, understand their functions, and use them effectively in reasoning and decision-making.*\n\n\n\n### **Bypassing LLM's mathematical computation limitations**\n\nThe `fibonacci_tool` function is a custom tool designed to compute the nth Fibonacci number. It is decorated with @tool, which allows it to be seamlessly integrated into an AI agent using the SmolAgents framework.\n\nLLMs, while powerful, often struggle with mathematical computations, especially recursive sequences like Fibonacci numbers. Instead of relying on the LLM's built-in reasoning (which often returns erroneous results even with the most powerful models like GPT-4o and o1 *try it for yourself*), this tool:\n\n1. Provides a deterministic and accurate calculation of Fibonacci numbers.\n2. Is significantly more efficient than recursive implementations, using an iterative approach to avoid excessive function calls.\n3. Ensures the AI agent always returns correct numerical results rather than relying on approximate or hallucinated values.\n<br>\n\n### **Bypassing the LLM's training date cutoff**\n\nFinally, we create a tool that leverages `DuckDuckGoSearch` to fetch up-to-date information from the web. \n\nThis tool empowers our agent with the ability to quickly retrieve relevant information on demand. Since models training have cutoff dates, this tool will allow us to bypass this cutoff limitation by researching the web for up to date answers to any query we may have.\n\n### Understanding the Final Answer Tool in AI Agents\n\nThe `FinalAnswerTool` is a specialized tool used in AI agent frameworks to consolidate the reasoning process and provide a definitive response. Unlike other tools that fetch data or perform computations, this tool acts as a final step where the AI commits to a structured output.\n\n#### Why This Tool Is Important for AI Agents\nAI agents often go through multi-step reasoning before arriving at an answer. The `FinalAnswerTool` helps in:\n\n1. Ensuring a structured final response\n    - Instead of returning intermediate steps, the agent commits to a well-defined answer.\n\n2. Improving interpretability\n    - By using a dedicated final output tool, responses become more consistent and easier to process.\n\n3. Integrating seamlessly into pipelines\n\n    - Many agent frameworks expect a clear final output formatâ€”this tool standardizes the output handling.\n\nWhen designing AI agents, tools like this help bridge the gap between reasoning and action, making interactions smoother and more reliable.\n\n### Picking our LLM and initializing it.\n\nTo instanciate our agent, we must first create a model instance by setting up a specific `TransformersModel` and its tokenizer (with CUDA enabled and controlled token limits). \n\n### Creating an AI Agent with Custom Tools\nNow that we've built our individual tools, the next step is assembling them into an AI agent. This is done using the `CodeAgent` class, which takes a language model (model), a set of tools, and other parameters to define its behavior.\n\n**Key Components of the AI Agent:**\n\n1. `Model` (model) â€“ The LLM that powers the agentâ€™s reasoning and decision-making.\n2. `Tools` (tools) â€“ A list of functions the agent can use to extend its capabilities.\n3. `Max Steps` (max_steps=6) â€“ Limits how many steps the agent can take before finalizing an answer.\n4. `Verbosity` (verbosity_level=1) â€“ Controls the level of detail in logs/debugging output.\n5. `Prompt Templates` (prompt_templates) â€“ Provides structured instructions for the agentâ€™s behavior.\n\nFinally, we query our `agent` by passing in our queries to the `.run()` method.\n","srcMarkdownNoYaml":"\n\n\n![V-LLM Generated Image](main.webp \"\")\n\n### Introduction: Why AI Agents Matter\n<br>\nBefore diving into the coding process, it's essential to understand the purpose of the program we aim to build. We need to identify the problem at hand and the business value we can derive from solving it...\n\nLol, who am I kidding, we're not at work here, so let's focus on picking problems that are ***fun*** to solve and let the marketing people worry about value creation.\n\nJokes aside, today our goal is to learn how to overcome the traditional limitations of Large Language Models (LLMs) by building Agents that are able to use *custom tools*.  \n\nWhile LLMs like GPT-4 are incredibly powerful and capable of performing various tasks such as answering questions, summarizing text, and reasoning about complex topics, they have **significant limitations** when used in isolation. What are those limitations, one may ask?\n\n\n1. **Lack of Real-Time Data**  \n   - LLMs are trained on static datasets and *cannot fetch live information* (e.g., stock prices, weather updates, or breaking news).  \n   - This leads to outdated or incomplete responses.  \n<br>\n2. **Inability to Perform Actions**  \n   - An LLM can tell you how to book a flight but *canâ€™t actually book it*â€”it lacks the ability to interact with external systems.  \n   - Agents, on the other hand, can call APIs, interact with databases, and automate tasks.  \n<br>\n3. **Limited Memory & Context**  \n   - While LLMs have a context window, they *forget past interactions beyond a limit* and struggle with long-term tasks.  \n   - Agents can store and retrieve relevant information, improving coherence over long workflows.  \n<br>\n4. **Mathematical & Computational Weaknesses**  \n   - LLMs can make **math mistakes** or fail in complex computations.  \n   - Agents can call external computation tools (e.g., a Python script or a calculator API) for precise results.  \n\n\n\n#### **How Agents Solve These Problems**  \n*AI agents* address many of the limitations of traditional language models by integrating external tools, allowing for greater accuracy, real-time data retrieval, and interaction with external systems. In this tutorial, we will explore how to create and integrate custom tools with LLMs to build more robust and functional AI agents. \n\nSpecifically, we will develop:\n\n- ðŸ§® Mathematical computation tools capable of producing precise results, overcoming the tendency of LLMs to generate incorrect numerical answers (with a clear example of ChatGPT's limitation that you can test against your Agent's tool) \n- ðŸ“ˆ Real-time stock market query tools to retrieve up-to-date financial information. \n- ðŸ” Web-search tools to access the latest research and trends, ensuring information is current. \n\nAll implementations will rely entirely on open-source libraries (big love to Hugging Face â¤ï¸) and will run locally on a standard machine with no more than 12GB of VRAM. While many existing tutorials focus on cloud-based implementations via [HuggingFace's Serverless API](https://huggingface.co/docs/api-inference/en/index), I want to demonstrate how to get things running without relying on the cloud.\n\n\n\n## Building Open Source Agents using HuggingFace's SmolAgents\n\n\nTo monitor GPU utilization in real-time, especially when running resource-intensive machine learning tasks, you can leverage the `nvidia-smi` command. On Ubuntu, simply uncomment and execute the following command in your terminal (provided that you have an Nvidia GPU):\n\n```bash\nwatch -n 1 nvidia-smi\n```\n\nThis command offers a live, updating snapshot of your GPU's performance, enabling you to monitor key metrics such as memory utilization and temperature. The `watch -n 1` component ensures that this information is refreshed every second. This is particularly useful for debugging, optimizing your machine learning workflows, and identifying specific commands or models that lead to critical \"Out Of Memory\" errors.\n\n### Imports & Setup\n\nIn this initial code block, we start by importing the necessary libraries to construct our local AI agent. Let's break down each import:\n\n- `typing`: We import `Any` from the `typing` module to provide type hints for our function signatures, which is a critical part of creating useful tools that our agents that use.\n\n- `smolagents`: From the `smolagents` library, we import:\n    - `Tool`: A base class for defining custom tools that our agent can use.\n    - `CodeAgent`: The core class for creating agents capable of generating and executing code.\n    - `TransformersModel`: A class to load and manage transformer models.\n    - `tool`: A decorator to easily register functions as tools for the agent.\n    - `DuckDuckGoSearchTool`: A pre-built tool that allows the agent to perform web searches using DuckDuckGo.\n- `transformers`: We import `AutoTokenizer` from the `transformers` library, which will be used to process text data for our models.\n- `yaml`: This library is imported to handle YAML configuration files, allowing us to define agent settings in a structured format.\n- `yfinance`: We import `yfinance` to fetch stock market data.\n- `datetime` and `pytz`: These are used for handling date and time information, which will be useful when working with financial data.\n\nThese imports set the stage for building a powerful AI agent that can interact with its environment through code execution and web searches.\n\n### Define your prompt template\nLoad up the adequate yaml prompt template. This one is compatible with the smolagents library and the model we will use. It was taken directly from [HuggingFace's Agents Course](https://huggingface.co/agents-course)\n\n### **Understanding AI Tool Creation with SmolAgents**\n\nWhen building AI agents, **defining tools properly is crucial** for ensuring smooth integration with frameworks like [SmolAgents](https://huggingface.co/docs/smolagents/en/index).  \nThe first tool we will build today uses the `yfinance` library to check the status (price) of a given ticker on the North American stock market. \n\n#### **Creating your first tool**\nThe function **`check_NA_market_status`** is decorated with `@tool`, marking it as a callable tool within the agent framework. This tool checks whether the stock market is open and returns the latest stock price if open, or the last closing price if closed.\n\n1. **Appropriate type-hinting is Essential**  \n   - The function takes a **single string argument (`ticker: str`)** and returns a **string (`-> str`)**.  \n   - This typing ensures that the agent understands the expected input and output format.  \n   - Without this, the function may not work correctly in an AI pipeline.\n\n2. **The Importance of a Docstring**  \n   - The function is documented with a **clear, structured docstring** explaining:  \n     - **What it does** (checks if the market is open and returns the appropriate stock price).  \n     - **What arguments it takes** (`ticker` â€“ the stock ticker symbol).  \n     - **What it returns** (a formatted string with market status and price).  \n   - AI agents rely on this information to understand the functionâ€™s role. When/if you build your own tool, it is critical to keep the same docstring template for your code to be compatible with the smolagents library.\n\n\n\n##### **Key Takeaways for developping tools**\n \n\n- **Follow the function template:** Use clear type hints and structured docstrings.  \n- **Ensure proper decoration:** The `@tool` decorator registers the function within the SmolAgents framework.  \n- **Keep return types simple:** AI models parse these results, so returning formatted strings helps with interpretability.  \n\nThis structured approach makes it easier for Agents to *call tools correctly, understand their functions, and use them effectively in reasoning and decision-making.*\n\n\n\n### **Bypassing LLM's mathematical computation limitations**\n\nThe `fibonacci_tool` function is a custom tool designed to compute the nth Fibonacci number. It is decorated with @tool, which allows it to be seamlessly integrated into an AI agent using the SmolAgents framework.\n\nLLMs, while powerful, often struggle with mathematical computations, especially recursive sequences like Fibonacci numbers. Instead of relying on the LLM's built-in reasoning (which often returns erroneous results even with the most powerful models like GPT-4o and o1 *try it for yourself*), this tool:\n\n1. Provides a deterministic and accurate calculation of Fibonacci numbers.\n2. Is significantly more efficient than recursive implementations, using an iterative approach to avoid excessive function calls.\n3. Ensures the AI agent always returns correct numerical results rather than relying on approximate or hallucinated values.\n<br>\n\n### **Bypassing the LLM's training date cutoff**\n\nFinally, we create a tool that leverages `DuckDuckGoSearch` to fetch up-to-date information from the web. \n\nThis tool empowers our agent with the ability to quickly retrieve relevant information on demand. Since models training have cutoff dates, this tool will allow us to bypass this cutoff limitation by researching the web for up to date answers to any query we may have.\n\n### Understanding the Final Answer Tool in AI Agents\n\nThe `FinalAnswerTool` is a specialized tool used in AI agent frameworks to consolidate the reasoning process and provide a definitive response. Unlike other tools that fetch data or perform computations, this tool acts as a final step where the AI commits to a structured output.\n\n#### Why This Tool Is Important for AI Agents\nAI agents often go through multi-step reasoning before arriving at an answer. The `FinalAnswerTool` helps in:\n\n1. Ensuring a structured final response\n    - Instead of returning intermediate steps, the agent commits to a well-defined answer.\n\n2. Improving interpretability\n    - By using a dedicated final output tool, responses become more consistent and easier to process.\n\n3. Integrating seamlessly into pipelines\n\n    - Many agent frameworks expect a clear final output formatâ€”this tool standardizes the output handling.\n\nWhen designing AI agents, tools like this help bridge the gap between reasoning and action, making interactions smoother and more reliable.\n\n### Picking our LLM and initializing it.\n\nTo instanciate our agent, we must first create a model instance by setting up a specific `TransformersModel` and its tokenizer (with CUDA enabled and controlled token limits). \n\n### Creating an AI Agent with Custom Tools\nNow that we've built our individual tools, the next step is assembling them into an AI agent. This is done using the `CodeAgent` class, which takes a language model (model), a set of tools, and other parameters to define its behavior.\n\n**Key Components of the AI Agent:**\n\n1. `Model` (model) â€“ The LLM that powers the agentâ€™s reasoning and decision-making.\n2. `Tools` (tools) â€“ A list of functions the agent can use to extend its capabilities.\n3. `Max Steps` (max_steps=6) â€“ Limits how many steps the agent can take before finalizing an answer.\n4. `Verbosity` (verbosity_level=1) â€“ Controls the level of detail in logs/debugging output.\n5. `Prompt Templates` (prompt_templates) â€“ Provides structured instructions for the agentâ€™s behavior.\n\nFinally, we query our `agent` by passing in our queries to the `.run()` method.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"github-dark","css":["../../styles.css"],"toc":true,"output-file":"local_agent.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","theme":["cosmo","brand"],"title-block-banner":true,"title-block-style":"default","code-copy":true,"toc-location":"right","author":{"name":"Jonathan Moatti"},"title":"Building Agents with Tools Locally","date":"2025-02-21","description":"Building an AI Agent with tools locally using HuggingFace SmolAgents and Transformers Libraries","categories":["tutorial","code"],"image":"thumbnail.webp","image-alt":"V-LLM Generated Image","page-layout":"full","max-width":"1300px"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}